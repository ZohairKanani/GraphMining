{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab7655d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# enhanced_graphsage_optimized_auc_10percent_tuned_l2.py\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import networkx as nx\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.metrics import f1_score, confusion_matrix, roc_curve, auc, precision_recall_curve, average_precision_score\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "from tqdm import tqdm\n",
    "import os\n",
    "from collections import Counter, deque\n",
    "import random\n",
    "import math\n",
    "\n",
    "# Set environment variable to reduce memory fragmentation\n",
    "os.environ['PYTORCH_CUDA_ALLOC_CONF'] = 'expandable_segments:True'\n",
    "\n",
    "print(\"ðŸ” Checking GPU availability...\")\n",
    "print(f\"PyTorch version: {torch.__version__}\")\n",
    "print(f\"CUDA available: {torch.cuda.is_available()}\")\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    device = torch.device('cuda')\n",
    "    print(f\"âœ… Using GPU: {torch.cuda.get_device_name(0)}\")\n",
    "    print(f\"âœ… CUDA version: {torch.version.cuda}\")\n",
    "    print(f\"âœ… GPU Memory: {torch.cuda.get_device_properties(0).total_memory / 1024**3:.1f} GB\")\n",
    "else:\n",
    "    device = torch.device('cpu')\n",
    "    print(\"âŒ Using CPU - GPU not available\")\n",
    "    exit(1)\n",
    "\n",
    "# Install PyTorch Geometric if not available\n",
    "try:\n",
    "    from torch_geometric.data import Data\n",
    "    from torch_geometric.nn import SAGEConv\n",
    "    print(\"âœ… PyTorch Geometric imports successful\")\n",
    "except ImportError:\n",
    "    print(\"ðŸ“¦ Installing PyTorch Geometric...\")\n",
    "    !pip install torch-scatter torch-sparse torch-cluster torch-spline-conv torch-geometric -f https://data.pyg.org/whl/torch-{torch.__version__}.html\n",
    "    from torch_geometric.data import Data\n",
    "    from torch_geometric.nn import SAGEConv\n",
    "    print(\"âœ… PyTorch Geometric installed successfully\")\n",
    "\n",
    "class EnhancedGraphSAGE3L(nn.Module):\n",
    "    def __init__(self, in_channels, hidden_channels, out_channels, dropout=0.2):\n",
    "        super(EnhancedGraphSAGE3L, self).__init__()\n",
    "        self.conv1 = SAGEConv(in_channels, hidden_channels)\n",
    "        self.conv2 = SAGEConv(hidden_channels, hidden_channels)\n",
    "        self.conv3 = SAGEConv(hidden_channels, out_channels)\n",
    "\n",
    "        self.bn1 = nn.BatchNorm1d(hidden_channels)\n",
    "        self.bn2 = nn.BatchNorm1d(hidden_channels)\n",
    "\n",
    "        self.dropout = dropout\n",
    "        self.residual = True\n",
    "\n",
    "    def forward(self, x, edge_index):\n",
    "        # First layer\n",
    "        x1 = self.conv1(x, edge_index)\n",
    "        x1 = self.bn1(x1)\n",
    "        x1 = F.relu(x1)\n",
    "        x1 = F.dropout(x1, p=self.dropout, training=self.training)\n",
    "\n",
    "        # Second layer with residual connection\n",
    "        x2 = self.conv2(x1, edge_index)\n",
    "        x2 = self.bn2(x2)\n",
    "        if self.residual:\n",
    "            x2 = x2 + x1  # Residual connection\n",
    "        x2 = F.relu(x2)\n",
    "        x2 = F.dropout(x2, p=self.dropout, training=self.training)\n",
    "\n",
    "        # Third layer\n",
    "        x3 = self.conv3(x2, edge_index)\n",
    "\n",
    "        return F.normalize(x3, p=2, dim=1)  # Normalize embeddings\n",
    "\n",
    "class AUCEnhancedLinkPredictor(nn.Module):\n",
    "    def __init__(self, in_channels, hidden_dim=128, dropout=0.3):\n",
    "        super(AUCEnhancedLinkPredictor, self).__init__()\n",
    "\n",
    "        # Fixed architecture - correct input dimension\n",
    "        # Input: src (in_channels) + dst (in_channels) + 3 similarity features = 2*in_channels + 3\n",
    "        input_dim = 2 * in_channels + 3\n",
    "\n",
    "        self.lin1 = nn.Linear(input_dim, hidden_dim)\n",
    "        self.lin2 = nn.Linear(hidden_dim, hidden_dim)\n",
    "        self.lin3 = nn.Linear(hidden_dim, hidden_dim // 2)\n",
    "        self.lin4 = nn.Linear(hidden_dim // 2, 1)\n",
    "\n",
    "        self.bn1 = nn.BatchNorm1d(hidden_dim)\n",
    "        self.bn2 = nn.BatchNorm1d(hidden_dim)\n",
    "        self.bn3 = nn.BatchNorm1d(hidden_dim // 2)\n",
    "\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self._init_weights()\n",
    "\n",
    "    def _init_weights(self):\n",
    "        for module in self.modules():\n",
    "            if isinstance(module, nn.Linear):\n",
    "                nn.init.xavier_uniform_(module.weight)\n",
    "                nn.init.constant_(module.bias, 0.1)\n",
    "\n",
    "    def forward(self, z, edge_index):\n",
    "        src = z[edge_index[0]]\n",
    "        dst = z[edge_index[1]]\n",
    "\n",
    "        # Enhanced feature combination for better AUC\n",
    "        # Method 1: Multiple similarity measures\n",
    "        dot_product = (src * dst).sum(dim=1, keepdim=True)\n",
    "        cosine_sim = F.cosine_similarity(src, dst, dim=1).unsqueeze(1)\n",
    "        euclidean_dist = torch.norm(src - dst, dim=1, keepdim=True)\n",
    "        euclidean_sim = 1.0 / (1.0 + euclidean_dist)  # Convert distance to similarity\n",
    "\n",
    "        # Method 2: Enhanced MLP with more features\n",
    "        edge_features = torch.cat([src, dst, dot_product, cosine_sim, euclidean_sim], dim=1)\n",
    "\n",
    "        mlp_output = F.leaky_relu(self.bn1(self.lin1(edge_features)), negative_slope=0.1)\n",
    "        mlp_output = self.dropout(mlp_output)\n",
    "\n",
    "        mlp_output = F.leaky_relu(self.bn2(self.lin2(mlp_output)), negative_slope=0.1)\n",
    "        mlp_output = self.dropout(mlp_output)\n",
    "\n",
    "        mlp_output = F.leaky_relu(self.bn3(self.lin3(mlp_output)), negative_slope=0.1)\n",
    "        mlp_output = self.dropout(mlp_output)\n",
    "\n",
    "        mlp_score = self.lin4(mlp_output)\n",
    "\n",
    "        # Optimized combination for AUC\n",
    "        combined_score = mlp_score + 0.2 * dot_product + 0.2 * cosine_sim\n",
    "\n",
    "        return torch.sigmoid(combined_score).squeeze()\n",
    "\n",
    "def compute_auc_optimized_features(G, nodes):\n",
    "    \"\"\"Compute features optimized for AUC improvement - EXACT SAME AS 5% MODEL\"\"\"\n",
    "    print(\"Computing AUC-optimized features...\")\n",
    "\n",
    "    features = []\n",
    "\n",
    "    # Precompute essential metrics\n",
    "    print(\"  - Computing basic graph metrics...\")\n",
    "    in_degrees = dict(G.in_degree())\n",
    "    out_degrees = dict(G.out_degree())\n",
    "    total_degrees = {node: in_degrees.get(node, 0) + out_degrees.get(node, 0) for node in nodes}\n",
    "\n",
    "    print(\"  - Computing PageRank...\")\n",
    "    pagerank = nx.pagerank(G, alpha=0.85, max_iter=50)\n",
    "\n",
    "    print(\"  - Computing clustering coefficients...\")\n",
    "    clustering = nx.clustering(G)\n",
    "\n",
    "    print(\"  - Computing betweenness centrality (sampled)...\")\n",
    "    # Sample nodes for betweenness to save memory\n",
    "    sample_nodes = random.sample(nodes, min(1000, len(nodes)))\n",
    "    betweenness = nx.betweenness_centrality_subset(G, sample_nodes, sample_nodes)\n",
    "\n",
    "    print(\"  - Computing AUC-optimized features...\")\n",
    "\n",
    "    for i, node in enumerate(tqdm(nodes, desc=\"Computing AUC features\")):\n",
    "        node_feats = []\n",
    "\n",
    "        # 1. Enhanced Degree Features\n",
    "        in_deg = in_degrees.get(node, 0)\n",
    "        out_deg = out_degrees.get(node, 0)\n",
    "        total_deg = total_degrees.get(node, 0)\n",
    "\n",
    "        node_feats.extend([\n",
    "            in_deg, out_deg, total_deg,\n",
    "            np.log1p(in_deg), np.log1p(out_deg),\n",
    "            np.log1p(total_deg),\n",
    "            in_deg / (total_deg + 1e-8),\n",
    "            out_deg / (total_deg + 1e-8),\n",
    "        ])\n",
    "\n",
    "        # 2. Enhanced Centrality Measures\n",
    "        node_feats.extend([\n",
    "            pagerank.get(node, 0),\n",
    "            clustering.get(node, 0),\n",
    "            betweenness.get(node, 0),\n",
    "        ])\n",
    "\n",
    "        # 3. Neighborhood Structure Features\n",
    "        successors = list(G.successors(node))\n",
    "        predecessors = list(G.predecessors(node))\n",
    "\n",
    "        # Enhanced neighbor statistics\n",
    "        if successors:\n",
    "            succ_degrees = [total_degrees.get(n, 0) for n in successors[:15]]\n",
    "            node_feats.extend([\n",
    "                np.mean(succ_degrees),\n",
    "                np.std(succ_degrees),\n",
    "                np.max(succ_degrees),\n",
    "            ])\n",
    "        else:\n",
    "            node_feats.extend([0, 0, 0])\n",
    "\n",
    "        if predecessors:\n",
    "            pred_degrees = [total_degrees.get(n, 0) for n in predecessors[:15]]\n",
    "            node_feats.extend([\n",
    "                np.mean(pred_degrees),\n",
    "                np.std(pred_degrees),\n",
    "                np.max(pred_degrees),\n",
    "            ])\n",
    "        else:\n",
    "            node_feats.extend([0, 0, 0])\n",
    "\n",
    "        # 4. Enhanced Reciprocity Features\n",
    "        reciprocal_count = sum(1 for succ in successors[:25] if G.has_edge(succ, node))\n",
    "        node_feats.extend([\n",
    "            reciprocal_count,\n",
    "            reciprocal_count / (out_deg + 1e-8) if out_deg > 0 else 0,\n",
    "        ])\n",
    "\n",
    "        # 5. Structural Balance Features\n",
    "        node_feats.extend([\n",
    "            min(len(successors), len(predecessors)) / (max(len(successors), len(predecessors)) + 1e-8),\n",
    "            abs(len(successors) - len(predecessors)),\n",
    "        ])\n",
    "\n",
    "        features.append(node_feats)\n",
    "\n",
    "    feature_array = np.array(features)\n",
    "    print(f\"AUC-optimized feature matrix shape: {feature_array.shape}\")\n",
    "    return feature_array\n",
    "\n",
    "def print_graph_info(G):\n",
    "    \"\"\"Print graph information - EXACT SAME AS 5% MODEL\"\"\"\n",
    "    print(f\"  - Type: {type(G)}\")\n",
    "    print(f\"  - Number of nodes: {G.number_of_nodes()}\")\n",
    "    print(f\"  - Number of edges: {G.number_of_edges()}\")\n",
    "    if hasattr(G, 'is_directed') and G.is_directed():\n",
    "        print(f\"  - Directed: Yes\")\n",
    "        in_degrees = [d for n, d in G.in_degree()]\n",
    "        out_degrees = [d for n, d in G.out_degree()]\n",
    "        print(f\"  - Average in-degree: {np.mean(in_degrees):.2f}\")\n",
    "        print(f\"  - Average out-degree: {np.mean(out_degrees):.2f}\")\n",
    "\n",
    "        try:\n",
    "            reciprocity = nx.reciprocity(G)\n",
    "            print(f\"  - Reciprocity: {reciprocity:.4f}\")\n",
    "        except:\n",
    "            print(f\"  - Reciprocity: N/A\")\n",
    "\n",
    "        density = nx.density(G)\n",
    "        print(f\"  - Density: {density:.6f}\")\n",
    "\n",
    "        weakly_connected = nx.number_weakly_connected_components(G)\n",
    "        print(f\"  - Weakly connected components: {weakly_connected}\")\n",
    "\n",
    "        try:\n",
    "            avg_clustering = nx.average_clustering(G)\n",
    "            print(f\"  - Average clustering: {avg_clustering:.4f}\")\n",
    "        except:\n",
    "            print(f\"  - Average clustering: N/A\")\n",
    "\n",
    "def load_and_preprocess_auc_optimized(sample_fraction=0.10):\n",
    "    \"\"\"AUC-optimized data loading and preprocessing - EXACT SAME AS 5% MODEL\"\"\"\n",
    "    print(\"ðŸ“‚ Loading and preprocessing AUC-optimized data...\")\n",
    "\n",
    "    try:\n",
    "        df = pd.read_csv('train.csv')\n",
    "        print(f\"âœ… Train data loaded: {len(df)} edges\")\n",
    "    except FileNotFoundError:\n",
    "        print(\"âŒ Error: train.csv not found in current directory\")\n",
    "        print(\"ðŸ“ Files in current directory:\", os.listdir('.'))\n",
    "        exit(1)\n",
    "\n",
    "    df = df.sample(frac=sample_fraction, random_state=42)\n",
    "    print(f\"Using {len(df)} edges ({sample_fraction*100}% of data)\")\n",
    "\n",
    "    # Create graph\n",
    "    G = nx.DiGraph()\n",
    "    edges = list(zip(df['source_node'], df['destination_node']))\n",
    "    G.add_edges_from(edges)\n",
    "\n",
    "    print(f\"Graph info:\")\n",
    "    print_graph_info(G)\n",
    "\n",
    "    # Create node mapping\n",
    "    nodes = list(G.nodes())\n",
    "    node_mapping = {node: idx for idx, node in enumerate(nodes)}\n",
    "\n",
    "    # Compute AUC-optimized node features\n",
    "    node_feature_matrix = compute_auc_optimized_features(G, nodes)\n",
    "    print(f\"AUC-optimized feature matrix shape: {node_feature_matrix.shape}\")\n",
    "\n",
    "    # Normalize features\n",
    "    scaler = StandardScaler()\n",
    "    node_feature_matrix = scaler.fit_transform(node_feature_matrix)\n",
    "\n",
    "    # Create edge index for PyG\n",
    "    edge_index = []\n",
    "    for edge in G.edges():\n",
    "        src_idx = node_mapping[edge[0]]\n",
    "        dst_idx = node_mapping[edge[1]]\n",
    "        edge_index.append([src_idx, dst_idx])\n",
    "\n",
    "    edge_index = torch.tensor(edge_index, dtype=torch.long).t().contiguous()\n",
    "\n",
    "    # Convert to tensors and move to GPU immediately\n",
    "    x = torch.tensor(node_feature_matrix, dtype=torch.float).to(device)\n",
    "    edge_index = edge_index.to(device)\n",
    "\n",
    "    # Create Data object\n",
    "    data = Data(x=x, edge_index=edge_index)\n",
    "    data.num_nodes = len(nodes)\n",
    "\n",
    "    return data, G, node_mapping\n",
    "\n",
    "def train_test_split_edges_auc_optimized(data, val_ratio=0.05, test_ratio=0.1):\n",
    "    \"\"\"AUC-optimized edge splitting - EXACT SAME AS 5% MODEL\"\"\"\n",
    "    num_nodes = data.num_nodes\n",
    "    row, col = data.edge_index\n",
    "\n",
    "    # Remove self-loops\n",
    "    mask = row != col\n",
    "    row, col = row[mask], col[mask]\n",
    "\n",
    "    # Return random split\n",
    "    num_edges = row.size(0)\n",
    "    num_val = int(num_edges * val_ratio)\n",
    "    num_test = int(num_edges * test_ratio)\n",
    "\n",
    "    perm = torch.randperm(num_edges, device=device)\n",
    "    row, col = row[perm], col[perm]\n",
    "\n",
    "    # Split edges\n",
    "    test_edge_index = torch.stack([row[:num_test], col[:num_test]], dim=0)\n",
    "    val_edge_index = torch.stack([row[num_test:num_test+num_val], col[num_test:num_test+num_val]], dim=0)\n",
    "    train_edge_index = torch.stack([row[num_test+num_val:], col[num_test+num_val:]], dim=0)\n",
    "\n",
    "    def auc_optimized_negative_sampling(num_nodes, num_neg_samples):\n",
    "        \"\"\"AUC-optimized negative sampling\"\"\"\n",
    "        num_neg_samples = min(num_neg_samples, 8000)\n",
    "        neg_edges = torch.randint(0, num_nodes, (2, num_neg_samples), device=device)\n",
    "\n",
    "        # Ensure no self-loops\n",
    "        self_loop_mask = neg_edges[0] == neg_edges[1]\n",
    "        if self_loop_mask.any():\n",
    "            for i in range(neg_edges.size(1)):\n",
    "                if neg_edges[0, i] == neg_edges[1, i]:\n",
    "                    new_dst = torch.randint(0, num_nodes, (1,), device=device)\n",
    "                    while new_dst == neg_edges[0, i]:\n",
    "                        new_dst = torch.randint(0, num_nodes, (1,), device=device)\n",
    "                    neg_edges[1, i] = new_dst\n",
    "\n",
    "        return neg_edges\n",
    "\n",
    "    test_neg_edge_index = auc_optimized_negative_sampling(num_nodes, num_test)\n",
    "    val_neg_edge_index = auc_optimized_negative_sampling(num_nodes, num_val)\n",
    "\n",
    "    data.train_pos_edge_index = train_edge_index\n",
    "    data.val_pos_edge_index = val_edge_index\n",
    "    data.val_neg_edge_index = val_neg_edge_index\n",
    "    data.test_pos_edge_index = test_edge_index\n",
    "    data.test_neg_edge_index = test_neg_edge_index\n",
    "\n",
    "    print(f\"Training edges: {train_edge_index.size(1)}\")\n",
    "    print(f\"Validation edges: {val_edge_index.size(1)}\")\n",
    "    print(f\"Test edges: {test_edge_index.size(1)}\")\n",
    "\n",
    "    return data\n",
    "\n",
    "def simple_negative_sampling(z, data, num_samples):\n",
    "    \"\"\"Simple negative sampling - EXACT SAME AS 5% MODEL\"\"\"\n",
    "    num_nodes = z.size(0)\n",
    "    num_samples = min(num_samples, 3000)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        neg_src = torch.randint(0, num_nodes, (num_samples,), device=device)\n",
    "        neg_dst = torch.randint(0, num_nodes, (num_samples,), device=device)\n",
    "\n",
    "        valid_mask = neg_src != neg_dst\n",
    "        neg_src = neg_src[valid_mask][:num_samples]\n",
    "        neg_dst = neg_dst[valid_mask][:num_samples]\n",
    "\n",
    "        if neg_src.numel() == 0:\n",
    "            neg_src = torch.randint(0, num_nodes, (num_samples,), device=device)\n",
    "            neg_dst = torch.randint(0, num_nodes, (num_samples,), device=device)\n",
    "            for i in range(num_samples):\n",
    "                while neg_src[i] == neg_dst[i]:\n",
    "                    neg_dst[i] = torch.randint(0, num_nodes, (1,), device=device)\n",
    "\n",
    "        return torch.stack([neg_src, neg_dst])\n",
    "\n",
    "def train_auc_optimized(model, predictor, data, epochs=100, l2_lambda=1e-5):\n",
    "    \"\"\"ENHANCED training for 10% data with L2 regularization - FIXED MODEL LOADING\"\"\"\n",
    "    model = model.to(device)\n",
    "    predictor = predictor.to(device)\n",
    "    data = data.to(device)\n",
    "\n",
    "    # TUNED optimizer for 10% data\n",
    "    optimizer = torch.optim.AdamW(\n",
    "        list(model.parameters()) + list(predictor.parameters()),\n",
    "        lr=0.001,  # Slightly higher learning rate for faster convergence\n",
    "        weight_decay=1e-5\n",
    "    )\n",
    "\n",
    "    # TUNED scheduler for 10% data\n",
    "    scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(\n",
    "        optimizer, mode='max', factor=0.5, patience=15\n",
    "    )\n",
    "\n",
    "    train_losses = []\n",
    "    val_f1_scores = []\n",
    "    val_auc_scores = []\n",
    "\n",
    "    best_val_auc = 0.0\n",
    "    patience = 25  # Increased patience for 10% data\n",
    "    patience_counter = 0\n",
    "\n",
    "    model.train()\n",
    "    predictor.train()\n",
    "\n",
    "    torch.cuda.empty_cache()\n",
    "\n",
    "    pbar = tqdm(range(epochs), desc=\"Tuned AUC-Optimized Training with L2\")\n",
    "    for epoch in pbar:\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # Get node embeddings\n",
    "        z = model(data.x, data.train_pos_edge_index)\n",
    "\n",
    "        # Positive edges loss\n",
    "        pos_out = predictor(z, data.train_pos_edge_index)\n",
    "        pos_loss = F.binary_cross_entropy(pos_out, torch.ones_like(pos_out))\n",
    "\n",
    "        # Negative sampling - INCREASED for 10% data\n",
    "        neg_samples = min(data.train_pos_edge_index.size(1) // 2, 3500)  # Increased from 2500\n",
    "        neg_edge_index = simple_negative_sampling(z, data, neg_samples)\n",
    "\n",
    "        if neg_edge_index.size(1) > 0:\n",
    "            neg_out = predictor(z, neg_edge_index)\n",
    "            neg_loss = F.binary_cross_entropy(neg_out, torch.zeros_like(neg_out))\n",
    "        else:\n",
    "            neg_loss = torch.tensor(0.0, device=device)\n",
    "\n",
    "        # Combined BCE loss\n",
    "        bce_loss = pos_loss + neg_loss\n",
    "\n",
    "        # Add L2 regularization for both model and predictor\n",
    "        l2_reg = torch.tensor(0., device=device)\n",
    "        for param in model.parameters():\n",
    "            l2_reg += torch.norm(param, p=2)\n",
    "        for param in predictor.parameters():\n",
    "            l2_reg += torch.norm(param, p=2)\n",
    "\n",
    "        # Total loss with L2 regularization\n",
    "        loss = bce_loss + l2_lambda * l2_reg\n",
    "\n",
    "        loss.backward()\n",
    "\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
    "        torch.nn.utils.clip_grad_norm_(predictor.parameters(), max_norm=1.0)\n",
    "\n",
    "        optimizer.step()\n",
    "\n",
    "        train_losses.append(loss.item())\n",
    "\n",
    "        # Validation every 3 epochs\n",
    "        if epoch % 3 == 0 or epoch == epochs - 1:\n",
    "            with torch.no_grad():\n",
    "                model.eval()\n",
    "                predictor.eval()\n",
    "\n",
    "                z_val = model(data.x, data.train_pos_edge_index)\n",
    "\n",
    "                # Process validation in batches\n",
    "                batch_size = 6000\n",
    "                val_preds = []\n",
    "                val_trues = []\n",
    "\n",
    "                # Positive validation edges\n",
    "                for i in range(0, data.val_pos_edge_index.size(1), batch_size):\n",
    "                    end_idx = min(i + batch_size, data.val_pos_edge_index.size(1))\n",
    "                    batch_edges = data.val_pos_edge_index[:, i:end_idx]\n",
    "                    batch_pred = predictor(z_val, batch_edges)\n",
    "                    val_preds.append(batch_pred)\n",
    "                    val_trues.append(torch.ones_like(batch_pred))\n",
    "\n",
    "                # Negative validation edges\n",
    "                for i in range(0, data.val_neg_edge_index.size(1), batch_size):\n",
    "                    end_idx = min(i + batch_size, data.val_neg_edge_index.size(1))\n",
    "                    batch_edges = data.val_neg_edge_index[:, i:end_idx]\n",
    "                    batch_pred = predictor(z_val, batch_edges)\n",
    "                    val_preds.append(batch_pred)\n",
    "                    val_trues.append(torch.zeros_like(batch_pred))\n",
    "\n",
    "                val_pred = torch.cat(val_preds)\n",
    "                val_true = torch.cat(val_trues)\n",
    "                val_pred_binary = (val_pred > 0.5).float()\n",
    "                val_f1 = f1_score(val_true.cpu().numpy(), val_pred_binary.cpu().numpy())\n",
    "\n",
    "                # Calculate AUC\n",
    "                val_pred_np = val_pred.cpu().numpy()\n",
    "                val_true_np = val_true.cpu().numpy()\n",
    "                fpr, tpr, _ = roc_curve(val_true_np, val_pred_np)\n",
    "                val_auc = auc(fpr, tpr)\n",
    "\n",
    "                val_f1_scores.append(val_f1)\n",
    "                val_auc_scores.append(val_auc)\n",
    "\n",
    "                # Update learning rate\n",
    "                scheduler.step(val_auc)\n",
    "\n",
    "                model.train()\n",
    "                predictor.train()\n",
    "\n",
    "            # Early stopping based on AUC with TUNED parameters\n",
    "            if epoch >= 20:  # Increased minimum epochs for 10% data\n",
    "                if val_auc > best_val_auc:\n",
    "                    best_val_auc = val_auc\n",
    "                    patience_counter = 0\n",
    "                    torch.save({\n",
    "                        'model_state_dict': model.state_dict(),\n",
    "                        'predictor_state_dict': predictor.state_dict(),\n",
    "                        'l2_lambda': l2_lambda,\n",
    "                        'best_val_auc': best_val_auc\n",
    "                    }, 'best_auc_optimized_model_10pct_tuned_l2.pth')\n",
    "                else:\n",
    "                    patience_counter += 1\n",
    "\n",
    "                if patience_counter >= patience:\n",
    "                    print(f\"ðŸ›‘ Early stopping at epoch {epoch}\")\n",
    "                    break\n",
    "\n",
    "        current_lr = optimizer.param_groups[0]['lr']\n",
    "        current_memory = torch.cuda.memory_allocated() / 1024**3\n",
    "\n",
    "        pbar.set_postfix({\n",
    "            'Train Loss': f'{loss.item():.4f}',\n",
    "            'BCE Loss': f'{bce_loss.item():.4f}',\n",
    "            'L2 Reg': f'{(l2_lambda * l2_reg).item():.6f}',\n",
    "            'Val F1': f'{val_f1 if (epoch % 3 == 0 or epoch == epochs - 1) else val_f1_scores[-1] if val_f1_scores else 0:.4f}',\n",
    "            'Val AUC': f'{val_auc if (epoch % 3 == 0 or epoch == epochs - 1) else val_auc_scores[-1] if val_auc_scores else 0:.4f}',\n",
    "            'LR': f'{current_lr:.6f}',\n",
    "            'GPU Mem': f'{current_memory:.2f}GB'\n",
    "        })\n",
    "\n",
    "        if epoch % 10 == 0:\n",
    "            torch.cuda.empty_cache()\n",
    "\n",
    "    # Load best model - FIXED: Use weights_only=False to handle NumPy scalars\n",
    "    if os.path.exists('best_auc_optimized_model_10pct_tuned_l2.pth'):\n",
    "        checkpoint = torch.load('best_auc_optimized_model_10pct_tuned_l2.pth', map_location=device, weights_only=False)\n",
    "        model.load_state_dict(checkpoint['model_state_dict'])\n",
    "        predictor.load_state_dict(checkpoint['predictor_state_dict'])\n",
    "        print(f\"âœ… Loaded best model with validation AUC: {checkpoint['best_val_auc']:.4f}\")\n",
    "        print(f\"âœ… L2 Lambda used: {checkpoint.get('l2_lambda', l2_lambda)}\")\n",
    "\n",
    "    return train_losses, val_f1_scores, val_auc_scores\n",
    "\n",
    "def evaluate_auc_optimized(model, predictor, data):\n",
    "    \"\"\"AUC-optimized evaluation - EXACT SAME AS 5% MODEL\"\"\"\n",
    "    model.eval()\n",
    "    predictor.eval()\n",
    "\n",
    "    with torch.no_grad():\n",
    "        z = model(data.x, data.train_pos_edge_index)\n",
    "\n",
    "        batch_size = 6000\n",
    "        all_pred = []\n",
    "        all_true = []\n",
    "\n",
    "        # Process positive edges\n",
    "        pos_edge_index = data.test_pos_edge_index\n",
    "        for i in range(0, pos_edge_index.size(1), batch_size):\n",
    "            end_idx = min(i + batch_size, pos_edge_index.size(1))\n",
    "            batch_edge_index = pos_edge_index[:, i:end_idx]\n",
    "            pos_pred_batch = predictor(z, batch_edge_index)\n",
    "            all_pred.append(pos_pred_batch)\n",
    "            all_true.append(torch.ones_like(pos_pred_batch))\n",
    "\n",
    "        # Process negative edges\n",
    "        neg_edge_index = data.test_neg_edge_index\n",
    "        for i in range(0, neg_edge_index.size(1), batch_size):\n",
    "            end_idx = min(i + batch_size, neg_edge_index.size(1))\n",
    "            batch_edge_index = neg_edge_index[:, i:end_idx]\n",
    "            neg_pred_batch = predictor(z, batch_edge_index)\n",
    "            all_pred.append(neg_pred_batch)\n",
    "            all_true.append(torch.zeros_like(neg_pred_batch))\n",
    "\n",
    "        # Combine and move to CPU for metrics\n",
    "        all_pred_tensor = torch.cat(all_pred).cpu().numpy()\n",
    "        all_true_tensor = torch.cat(all_true).cpu().numpy()\n",
    "\n",
    "        # Calculate metrics\n",
    "        pred_binary = (all_pred_tensor > 0.5).astype(int)\n",
    "        f1 = f1_score(all_true_tensor, pred_binary)\n",
    "        cm = confusion_matrix(all_true_tensor, pred_binary)\n",
    "\n",
    "        # ROC and Precision-Recall\n",
    "        fpr, tpr, _ = roc_curve(all_true_tensor, all_pred_tensor)\n",
    "        roc_auc = auc(fpr, tpr)\n",
    "        precision, recall, _ = precision_recall_curve(all_true_tensor, all_pred_tensor)\n",
    "        avg_precision = average_precision_score(all_true_tensor, all_pred_tensor)\n",
    "\n",
    "    return all_pred_tensor, all_true_tensor, f1, cm, fpr, tpr, roc_auc, precision, recall, avg_precision\n",
    "\n",
    "# ALL VISUALIZATION FUNCTIONS REMAIN EXACTLY THE SAME AS BEFORE\n",
    "def create_enhanced_visualization(model, predictor, data, G, node_mapping, num_nodes=100):\n",
    "    \"\"\"Create enhanced visualization with more nodes and edges - EXACT SAME AS 5% MODEL\"\"\"\n",
    "    print(\"\\nðŸŽ¨ Generating enhanced visualization...\")\n",
    "\n",
    "    # Find connected components with edges\n",
    "    components_with_edges = []\n",
    "    for component in nx.weakly_connected_components(G):\n",
    "        subG_temp = G.subgraph(component)\n",
    "        if subG_temp.number_of_edges() > 0:  # Only consider components with edges\n",
    "            components_with_edges.append(component)\n",
    "        if len(components_with_edges) >= 10:  # Collect more options\n",
    "            break\n",
    "\n",
    "    if not components_with_edges:\n",
    "        print(\"âŒ No components with edges found for visualization\")\n",
    "        return None, None\n",
    "\n",
    "    # Sort by size and pick the largest component with edges\n",
    "    components_with_edges.sort(key=len, reverse=True)\n",
    "\n",
    "    # Try to find a component that meets our size requirements and has edges\n",
    "    selected_component = None\n",
    "    for component in components_with_edges:\n",
    "        subG_temp = G.subgraph(component)\n",
    "        if subG_temp.number_of_edges() > 0:  # Ensure it has edges\n",
    "            if len(component) >= num_nodes:\n",
    "                # Sample from large component while maintaining connectivity\n",
    "                # Start with a random node and expand via BFS to maintain connectivity\n",
    "                start_node = random.choice(list(component))\n",
    "                visited = set([start_node])\n",
    "                queue = deque([start_node])\n",
    "\n",
    "                while queue and len(visited) < num_nodes:\n",
    "                    current = queue.popleft()\n",
    "                    # Add neighbors that are in the component\n",
    "                    neighbors = set(G.successors(current)) | set(G.predecessors(current))\n",
    "                    neighbors = neighbors & component  # Only neighbors in this component\n",
    "                    for neighbor in neighbors:\n",
    "                        if neighbor not in visited and len(visited) < num_nodes:\n",
    "                            visited.add(neighbor)\n",
    "                            queue.append(neighbor)\n",
    "\n",
    "                selected_component = visited\n",
    "                break\n",
    "            elif len(component) >= 50:  # Accept components with at least 50 nodes\n",
    "                selected_component = component\n",
    "                break\n",
    "\n",
    "    # If no suitable component found, use the largest component with edges\n",
    "    if selected_component is None:\n",
    "        largest_component = components_with_edges[0]\n",
    "        subG_temp = G.subgraph(largest_component)\n",
    "        if subG_temp.number_of_edges() > 0:\n",
    "            selected_component = largest_component\n",
    "            # If component is too large, sample connected nodes\n",
    "            if len(selected_component) > num_nodes:\n",
    "                start_node = random.choice(list(selected_component))\n",
    "                visited = set([start_node])\n",
    "                queue = deque([start_node])\n",
    "\n",
    "                while queue and len(visited) < num_nodes:\n",
    "                    current = queue.popleft()\n",
    "                    neighbors = set(G.successors(current)) | set(G.predecessors(current))\n",
    "                    neighbors = neighbors & selected_component\n",
    "                    for neighbor in neighbors:\n",
    "                        if neighbor not in visited and len(visited) < num_nodes:\n",
    "                            visited.add(neighbor)\n",
    "                            queue.append(neighbor)\n",
    "                selected_component = visited\n",
    "\n",
    "    if selected_component is None:\n",
    "        print(\"âŒ Could not find suitable component with edges for visualization\")\n",
    "        return None, None\n",
    "\n",
    "    subG = G.subgraph(selected_component)\n",
    "\n",
    "    print(f\"Enhanced subgraph nodes: {subG.number_of_nodes()}, edges: {subG.number_of_edges()}\")\n",
    "\n",
    "    if subG.number_of_edges() == 0:\n",
    "        print(\"âŒ No edges in enhanced subgraph for visualization\")\n",
    "        return None, None\n",
    "\n",
    "    # Create mapping for subgraph nodes\n",
    "    subgraph_nodes = list(subG.nodes())\n",
    "    subgraph_node_indices = [node_mapping[node] for node in subgraph_nodes]\n",
    "\n",
    "    # Get embeddings for subgraph nodes\n",
    "    model.eval()\n",
    "    predictor.eval()\n",
    "    with torch.no_grad():\n",
    "        z_all = model(data.x, data.train_pos_edge_index)\n",
    "        z_subgraph = z_all[subgraph_node_indices]\n",
    "\n",
    "    # Create edge list for the subgraph\n",
    "    subgraph_edges = []\n",
    "    for edge in subG.edges():\n",
    "        src_idx = node_mapping[edge[0]]\n",
    "        dst_idx = node_mapping[edge[1]]\n",
    "        if src_idx in subgraph_node_indices and dst_idx in subgraph_node_indices:\n",
    "            local_src = subgraph_node_indices.index(src_idx)\n",
    "            local_dst = subgraph_node_indices.index(dst_idx)\n",
    "            subgraph_edges.append([local_src, local_dst])\n",
    "\n",
    "    if not subgraph_edges:\n",
    "        print(\"âŒ No valid edges in enhanced subgraph for visualization\")\n",
    "        return None, None\n",
    "\n",
    "    subgraph_edge_index = torch.tensor(subgraph_edges, dtype=torch.long).t().contiguous().to(device)\n",
    "\n",
    "    # Predict probabilities for subgraph edges in batches\n",
    "    edge_probs = []\n",
    "    batch_size = 1000\n",
    "    with torch.no_grad():\n",
    "        for i in range(0, subgraph_edge_index.size(1), batch_size):\n",
    "            end_idx = min(i + batch_size, subgraph_edge_index.size(1))\n",
    "            batch_edges = subgraph_edge_index[:, i:end_idx]\n",
    "            batch_probs = predictor(z_subgraph, batch_edges).cpu().numpy()\n",
    "            edge_probs.extend(batch_probs)\n",
    "\n",
    "    edge_probs = np.array(edge_probs)\n",
    "\n",
    "    # Create enhanced visualization\n",
    "    fig, ((ax1, ax2), (ax3, ax4)) = plt.subplots(2, 2, figsize=(20, 16))\n",
    "\n",
    "    # Plot 1: Network structure with edge probabilities\n",
    "    pos = nx.spring_layout(subG, k=2/np.sqrt(len(subG.nodes())), iterations=50)\n",
    "\n",
    "    # Node sizes based on degree with enhanced scaling\n",
    "    node_sizes = [30 + 15 * np.log1p(subG.degree(node)) for node in subG.nodes()]\n",
    "\n",
    "    # Edge colors and widths based on prediction probability\n",
    "    edge_colors = edge_probs\n",
    "    edge_widths = [0.5 + 3 * prob for prob in edge_probs]\n",
    "\n",
    "    # Draw the network\n",
    "    nodes = nx.draw_networkx_nodes(subG, pos, node_size=node_sizes,\n",
    "                                 node_color='lightblue', alpha=0.8, ax=ax1)\n",
    "    edges = nx.draw_networkx_edges(subG, pos, edge_color=edge_colors,\n",
    "                                 edge_cmap=plt.cm.RdYlBu, width=edge_widths,\n",
    "                                 alpha=0.7, ax=ax1)\n",
    "\n",
    "    # Label only high-degree nodes to avoid clutter\n",
    "    high_degree_nodes = sorted(subG.nodes(), key=lambda x: subG.degree(x), reverse=True)[:8]\n",
    "    labels = {node: f'{node}' for node in high_degree_nodes}\n",
    "    nx.draw_networkx_labels(subG, pos, labels, font_size=9, ax=ax1)\n",
    "\n",
    "    # Add colorbar for edge probabilities\n",
    "    sm = plt.cm.ScalarMappable(cmap=plt.cm.RdYlBu,\n",
    "                             norm=plt.Normalize(vmin=min(edge_colors), vmax=max(edge_colors)))\n",
    "    sm.set_array([])\n",
    "    cbar = plt.colorbar(sm, ax=ax1)\n",
    "    cbar.set_label('Edge Prediction Probability', fontweight='bold')\n",
    "\n",
    "    ax1.set_title(f'Enhanced Subgraph Visualization\\n({len(subG.nodes())} nodes, {len(subG.edges())} edges)', fontsize=14, fontweight='bold')\n",
    "    ax1.axis('off')\n",
    "\n",
    "    # Plot 2: Node embedding visualization (2D PCA)\n",
    "    from sklearn.decomposition import PCA\n",
    "\n",
    "    z_subgraph_np = z_subgraph.cpu().numpy()\n",
    "    if z_subgraph_np.shape[1] > 2:\n",
    "        pca = PCA(n_components=2)\n",
    "        z_2d = pca.fit_transform(z_subgraph_np)\n",
    "        explained_variance = pca.explained_variance_ratio_.sum()\n",
    "    else:\n",
    "        z_2d = z_subgraph_np\n",
    "        explained_variance = 1.0\n",
    "\n",
    "    # Color nodes by degree\n",
    "    degrees = [subG.degree(node) for node in subG.nodes()]\n",
    "\n",
    "    scatter = ax2.scatter(z_2d[:, 0], z_2d[:, 1], c=degrees,\n",
    "                         cmap='plasma', s=50, alpha=0.8)\n",
    "    plt.colorbar(scatter, ax=ax2, label='Node Degree')\n",
    "\n",
    "    # Label a few nodes in the embedding space\n",
    "    for i, node in enumerate(high_degree_nodes):\n",
    "        if node in subgraph_nodes:\n",
    "            idx = subgraph_nodes.index(node)\n",
    "            ax2.annotate(str(node), (z_2d[idx, 0], z_2d[idx, 1]),\n",
    "                        xytext=(5, 5), textcoords='offset points',\n",
    "                        fontsize=8, fontweight='bold',\n",
    "                        bbox=dict(boxstyle=\"round,pad=0.3\", fc=\"yellow\", alpha=0.7))\n",
    "\n",
    "    ax2.set_title(f'Node Embeddings (PCA)\\nExplained Variance: {explained_variance:.3f}', fontsize=14, fontweight='bold')\n",
    "    ax2.set_xlabel('PC1')\n",
    "    ax2.set_ylabel('PC2')\n",
    "    ax2.grid(True, alpha=0.3)\n",
    "\n",
    "    # Plot 3: Edge probability distribution\n",
    "    ax3.hist(edge_probs, bins=30, alpha=0.7, color='skyblue', edgecolor='black')\n",
    "    ax3.axvline(np.mean(edge_probs), color='red', linestyle='--', linewidth=2, label=f'Mean: {np.mean(edge_probs):.3f}')\n",
    "    ax3.set_xlabel('Edge Prediction Probability')\n",
    "    ax3.set_ylabel('Frequency')\n",
    "    ax3.set_title('Distribution of Edge Probabilities', fontsize=14, fontweight='bold')\n",
    "    ax3.legend()\n",
    "    ax3.grid(True, alpha=0.3)\n",
    "\n",
    "    # Plot 4: Degree distribution of subgraph\n",
    "    degree_sequence = sorted([d for n, d in subG.degree()], reverse=True)\n",
    "    ax4.hist(degree_sequence, bins=20, alpha=0.7, color='lightgreen', edgecolor='black')\n",
    "    ax4.set_xlabel('Degree')\n",
    "    ax4.set_ylabel('Frequency')\n",
    "    ax4.set_title('Degree Distribution of Subgraph', fontsize=14, fontweight='bold')\n",
    "    ax4.grid(True, alpha=0.3)\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.savefig('enhanced_auc_visualization_10pct_tuned_l2.png', dpi=300, bbox_inches='tight')\n",
    "    plt.show()\n",
    "\n",
    "    # Print enhanced subgraph statistics\n",
    "    print(f\"\\nðŸ“Š Enhanced Subgraph Statistics:\")\n",
    "    print(f\"   - Nodes: {len(subG.nodes())}\")\n",
    "    print(f\"   - Edges: {len(subG.edges())}\")\n",
    "    print(f\"   - Average degree: {np.mean(degree_sequence):.2f}\")\n",
    "    print(f\"   - Maximum degree: {np.max(degree_sequence)}\")\n",
    "    print(f\"   - Average edge probability: {np.mean(edge_probs):.4f}\")\n",
    "    print(f\"   - Std edge probability: {np.std(edge_probs):.4f}\")\n",
    "    print(f\"   - Density: {nx.density(subG):.6f}\")\n",
    "\n",
    "    return subG, edge_probs\n",
    "\n",
    "def plot_auc_enhanced_results(train_losses, val_f1_scores, val_auc_scores, fpr, tpr, roc_auc, precision, recall,\n",
    "                            avg_precision, cm, f1_score_val, figsize=(20, 12)):\n",
    "    \"\"\"Plot AUC-enhanced results - EXACT SAME AS 5% MODEL\"\"\"\n",
    "    fig, ((ax1, ax2, ax3), (ax4, ax5, ax6)) = plt.subplots(2, 3, figsize=figsize)\n",
    "\n",
    "    # 1. Training Loss\n",
    "    epochs = range(len(train_losses))\n",
    "    ax1.plot(epochs, train_losses, label='Training Loss', linewidth=2, color='blue')\n",
    "    ax1.set_title('Training Loss', fontsize=14, fontweight='bold')\n",
    "    ax1.set_xlabel('Epoch')\n",
    "    ax1.set_ylabel('Loss')\n",
    "    ax1.legend()\n",
    "    ax1.grid(True, alpha=0.3)\n",
    "\n",
    "    # 2. Validation F1 and AUC over time\n",
    "    if val_f1_scores and val_auc_scores:\n",
    "        val_epochs = [i * 3 for i in range(len(val_f1_scores))]\n",
    "        ax2.plot(val_epochs, val_f1_scores, label='Validation F1', linewidth=2, color='green')\n",
    "        ax2.plot(val_epochs, val_auc_scores, label='Validation AUC', linewidth=2, color='purple')\n",
    "        ax2.set_title('Validation Metrics Over Time', fontsize=14, fontweight='bold')\n",
    "        ax2.set_xlabel('Epoch')\n",
    "        ax2.set_ylabel('Score')\n",
    "        ax2.legend()\n",
    "        ax2.grid(True, alpha=0.3)\n",
    "        ax2.set_ylim(0, 1.0)\n",
    "\n",
    "    # 3. ROC Curve\n",
    "    ax3.plot(fpr, tpr, color='darkorange', lw=3, label=f'ROC curve (AUC = {roc_auc:.4f})')\n",
    "    ax3.plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--', alpha=0.5)\n",
    "    ax3.set_xlim([0.0, 1.0])\n",
    "    ax3.set_ylim([0.0, 1.05])\n",
    "    ax3.set_xlabel('False Positive Rate', fontweight='bold')\n",
    "    ax3.set_ylabel('True Positive Rate', fontweight='bold')\n",
    "    ax3.set_title('ROC Curve', fontsize=14, fontweight='bold')\n",
    "    ax3.legend(loc=\"lower right\")\n",
    "    ax3.grid(True, alpha=0.3)\n",
    "\n",
    "    # 4. Precision-Recall Curve\n",
    "    ax4.plot(recall, precision, color='green', lw=3,\n",
    "             label=f'Precision-Recall (AP = {avg_precision:.4f})')\n",
    "    ax4.set_xlim([0.0, 1.0])\n",
    "    ax4.set_ylim([0.0, 1.05])\n",
    "    ax4.set_xlabel('Recall', fontweight='bold')\n",
    "    ax4.set_ylabel('Precision', fontweight='bold')\n",
    "    ax4.set_title('Precision-Recall Curve', fontsize=14, fontweight='bold')\n",
    "    ax4.legend(loc=\"lower left\")\n",
    "    ax4.grid(True, alpha=0.3)\n",
    "\n",
    "    # 5. Confusion Matrix\n",
    "    cm_display = sns.heatmap(cm, annot=True, fmt='d', cmap='Blues',\n",
    "                            xticklabels=['Predicted Negative', 'Predicted Positive'],\n",
    "                            yticklabels=['Actual Negative', 'Actual Positive'],\n",
    "                            ax=ax5, annot_kws={\"size\": 12})\n",
    "    ax5.set_title('Confusion Matrix', fontsize=14, fontweight='bold')\n",
    "    ax5.set_xlabel('Predicted Label', fontweight='bold')\n",
    "    ax5.set_ylabel('True Label', fontweight='bold')\n",
    "\n",
    "    # 6. Performance Comparison\n",
    "    models = ['Original GraphSAGE', 'XGBoost', 'Tuned 10% GraphSAGE']\n",
    "    f1_scores = [0.517, 0.928, f1_score_val]\n",
    "    auc_scores = [0.785, 0.932, roc_auc]\n",
    "\n",
    "    x = np.arange(len(models))\n",
    "    width = 0.35\n",
    "\n",
    "    ax6.bar(x - width/2, f1_scores, width, label='F1-Score', color='lightblue', edgecolor='black')\n",
    "    ax6.bar(x + width/2, auc_scores, width, label='AUC-ROC', color='lightcoral', edgecolor='black')\n",
    "    ax6.set_xlabel('Models')\n",
    "    ax6.set_ylabel('Scores')\n",
    "    ax6.set_title('Performance Comparison', fontsize=14, fontweight='bold')\n",
    "    ax6.set_xticks(x)\n",
    "    ax6.set_xticklabels(models, rotation=45)\n",
    "    ax6.legend()\n",
    "    ax6.grid(True, alpha=0.3)\n",
    "    ax6.set_ylim(0, 1.0)\n",
    "\n",
    "    # Add value labels\n",
    "    for i, (f1, auc_val) in enumerate(zip(f1_scores, auc_scores)):\n",
    "        ax6.text(i - width/2, f1 + 0.01, f'{f1:.3f}', ha='center', va='bottom', fontweight='bold')\n",
    "        ax6.text(i + width/2, auc_val + 0.01, f'{auc_val:.3f}', ha='center', va='bottom', fontweight='bold')\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.savefig('auc_tuned_comprehensive_results_10pct_l2.png', dpi=300, bbox_inches='tight', facecolor='white')\n",
    "    plt.show()\n",
    "\n",
    "    # Create detailed performance analysis\n",
    "    fig2, (ax7, ax8) = plt.subplots(1, 2, figsize=(15, 6))\n",
    "\n",
    "    # Improvement analysis\n",
    "    improvement_f1 = ((f1_score_val - 0.517) / 0.517) * 100\n",
    "    improvement_auc = ((roc_auc - 0.785) / 0.785) * 100\n",
    "    improvement_vs_xgboost_auc = ((roc_auc - 0.932) / 0.932) * 100\n",
    "\n",
    "    categories = ['F1 vs Original', 'AUC vs Original', 'AUC vs XGBoost']\n",
    "    improvements = [improvement_f1, improvement_auc, improvement_vs_xgboost_auc]\n",
    "\n",
    "    bars = ax7.bar(categories, improvements,\n",
    "                  color=['lightgreen' if x >= 0 else 'lightcoral' for x in improvements],\n",
    "                  edgecolor='black', alpha=0.8)\n",
    "    ax7.set_ylabel('Improvement (%)')\n",
    "    ax7.set_title('Performance Improvement Analysis', fontsize=14, fontweight='bold')\n",
    "    ax7.grid(True, alpha=0.3, axis='y')\n",
    "\n",
    "    # Add value labels on bars\n",
    "    for bar, value in zip(bars, improvements):\n",
    "        height = bar.get_height()\n",
    "        ax7.text(bar.get_x() + bar.get_width()/2., height + (1 if height >= 0 else -5),\n",
    "                f'{value:+.2f}%', ha='center', va='bottom' if height >= 0 else 'top',\n",
    "                fontweight='bold', fontsize=11)\n",
    "\n",
    "    # Metric scores comparison\n",
    "    metrics = ['F1-Score', 'AUC-ROC', 'Avg Precision']\n",
    "    our_scores = [f1_score_val, roc_auc, avg_precision]\n",
    "    xgboost_scores = [0.928, 0.932, 0.0]  # Assuming XGBoost AP not available\n",
    "\n",
    "    x = np.arange(len(metrics))\n",
    "    width = 0.35\n",
    "\n",
    "    ax8.bar(x - width/2, our_scores, width, label='Tuned 10% GraphSAGE', color='skyblue', edgecolor='black')\n",
    "    ax8.bar(x + width/2, xgboost_scores, width, label='XGBoost', color='orange', edgecolor='black')\n",
    "    ax8.set_xlabel('Metrics')\n",
    "    ax8.set_ylabel('Scores')\n",
    "    ax8.set_title('Tuned vs XGBoost', fontsize=14, fontweight='bold')\n",
    "    ax8.set_xticks(x)\n",
    "    ax8.set_xticklabels(metrics)\n",
    "    ax8.legend()\n",
    "    ax8.grid(True, alpha=0.3)\n",
    "    ax8.set_ylim(0, 1.0)\n",
    "\n",
    "    # Add value labels\n",
    "    for i, (our, xgb) in enumerate(zip(our_scores, xgboost_scores)):\n",
    "        ax8.text(i - width/2, our + 0.01, f'{our:.3f}', ha='center', va='bottom', fontweight='bold')\n",
    "        if xgb > 0:\n",
    "            ax8.text(i + width/2, xgb + 0.01, f'{xgb:.3f}', ha='center', va='bottom', fontweight='bold')\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.savefig('auc_tuned_detailed_analysis_10pct_l2.png', dpi=300, bbox_inches='tight', facecolor='white')\n",
    "    plt.show()\n",
    "\n",
    "def run_tuned_auc_experiment_10pct():\n",
    "    \"\"\"Run TUNED AUC-enhanced experiment for 10% data with L2 regularization\"\"\"\n",
    "    print(f\"\\n{'='*70}\")\n",
    "    print(f\"ðŸš€ TUNED AUC-ENHANCED 3-Layer GraphSAGE - 10% Data (WITH L2 REGULARIZATION)\")\n",
    "    print(f\"{'='*70}\")\n",
    "\n",
    "    try:\n",
    "        # Clear GPU memory\n",
    "        torch.cuda.empty_cache()\n",
    "\n",
    "        # Load AUC-optimized data with 10% sample\n",
    "        data, G, node_mapping = load_and_preprocess_auc_optimized(sample_fraction=0.10)\n",
    "\n",
    "        # Split edges\n",
    "        data = train_test_split_edges_auc_optimized(data)\n",
    "\n",
    "        print(f\"AUC-optimized features dimension: {data.num_features}\")\n",
    "        print(f\"Initial GPU Memory: {torch.cuda.memory_allocated()/1024**3:.2f} GB\")\n",
    "\n",
    "        # Initialize TUNED model with increased capacity for 10% data\n",
    "        model = EnhancedGraphSAGE3L(\n",
    "            in_channels=data.num_features,\n",
    "            hidden_channels=128,   # INCREASED from 96 for 10% data\n",
    "            out_channels=64,      # INCREASED from 48 for 10% data\n",
    "            dropout=0.2\n",
    "        )\n",
    "\n",
    "        predictor = AUCEnhancedLinkPredictor(64, hidden_dim=128, dropout=0.3)\n",
    "\n",
    "        print(f\"Model parameters: {sum(p.numel() for p in model.parameters()):,}\")\n",
    "        print(f\"Predictor parameters: {sum(p.numel() for p in predictor.parameters()):,}\")\n",
    "        print(f\"Total parameters: {sum(p.numel() for p in model.parameters()) + sum(p.numel() for p in predictor.parameters()):,}\")\n",
    "\n",
    "        # Train with TUNED parameters and L2 regularization\n",
    "        print(\"\\n\" + \"=\"*50)\n",
    "        print(\"Starting Tuned AUC-Optimized Training with L2 Regularization...\")\n",
    "        print(\"=\"*50)\n",
    "        train_losses, val_f1_scores, val_auc_scores = train_auc_optimized(\n",
    "            model, predictor, data, epochs=100, l2_lambda=1e-5  # Added L2 regularization\n",
    "        )\n",
    "\n",
    "        # Evaluate\n",
    "        print(\"\\n\" + \"=\"*50)\n",
    "        print(\"Evaluating Tuned AUC-Enhanced Model with L2 Regularization...\")\n",
    "        print(\"=\"*50)\n",
    "        results = evaluate_auc_optimized(model, predictor, data)\n",
    "        all_pred, all_true, f1, cm, fpr, tpr, roc_auc, precision, recall, avg_precision = results\n",
    "\n",
    "        # Print results\n",
    "        print(f\"\\nðŸ“Š TUNED AUC-ENHANCED GraphSAGE Performance (10% Data WITH L2):\")\n",
    "        print(f\"   F1-Score: {f1:.4f}\")\n",
    "        print(f\"   AUC-ROC: {roc_auc:.4f}\")\n",
    "        print(f\"   Average Precision: {avg_precision:.4f}\")\n",
    "\n",
    "        # Compare with baselines\n",
    "        print(f\"\\nðŸ” Performance Comparison:\")\n",
    "        print(f\"   Original GraphSAGE F1: 0.517\")\n",
    "        print(f\"   Original GraphSAGE AUC: 0.785\")\n",
    "        print(f\"   XGBoost F1: 0.928\")\n",
    "        print(f\"   XGBoost AUC: 0.932\")\n",
    "        print(f\"   5% Data GraphSAGE F1: 0.860\")\n",
    "        print(f\"   5% Data GraphSAGE AUC: 0.723\")\n",
    "        print(f\"   Previous 10% Data F1: 0.757\")\n",
    "        print(f\"   Previous 10% Data AUC: 0.695\")\n",
    "        print(f\"   Tuned 10% Data GraphSAGE F1: {f1:.3f}\")\n",
    "        print(f\"   Tuned 10% Data GraphSAGE AUC: {roc_auc:.3f}\")\n",
    "\n",
    "        improvement_f1 = ((f1 - 0.517) / 0.517) * 100\n",
    "        improvement_auc = ((roc_auc - 0.785) / 0.785) * 100\n",
    "        improvement_vs_5pct_f1 = ((f1 - 0.860) / 0.860) * 100\n",
    "        improvement_vs_5pct_auc = ((roc_auc - 0.723) / 0.723) * 100\n",
    "        improvement_vs_prev_10pct_f1 = ((f1 - 0.757) / 0.757) * 100\n",
    "        improvement_vs_prev_10pct_auc = ((roc_auc - 0.695) / 0.695) * 100\n",
    "\n",
    "        print(f\"\\nðŸ“ˆ Improvement vs Original GraphSAGE:\")\n",
    "        print(f\"   F1-Score: {improvement_f1:+.2f}%\")\n",
    "        print(f\"   AUC-ROC: {improvement_auc:+.2f}%\")\n",
    "        print(f\"\\nðŸ“Š Improvement vs 5% Data Model:\")\n",
    "        print(f\"   F1-Score: {improvement_vs_5pct_f1:+.2f}%\")\n",
    "        print(f\"   AUC-ROC: {improvement_vs_5pct_auc:+.2f}%\")\n",
    "        print(f\"\\nðŸ“Š Improvement vs Previous 10% Model:\")\n",
    "        print(f\"   F1-Score: {improvement_vs_prev_10pct_f1:+.2f}%\")\n",
    "        print(f\"   AUC-ROC: {improvement_vs_prev_10pct_auc:+.2f}%\")\n",
    "\n",
    "        # Plot results\n",
    "        print(\"\\nðŸ“Š Generating comprehensive performance plots...\")\n",
    "        plot_auc_enhanced_results(train_losses, val_f1_scores, val_auc_scores, fpr, tpr, roc_auc,\n",
    "                                precision, recall, avg_precision, cm, f1)\n",
    "\n",
    "        # Generate enhanced visualization\n",
    "        print(\"\\nðŸŽ¨ Generating enhanced visualization...\")\n",
    "        visualization_result = create_enhanced_visualization(model, predictor, data, G, node_mapping, num_nodes=100)\n",
    "\n",
    "        # Save all results and data\n",
    "        print(\"\\nðŸ’¾ Saving all results and data...\")\n",
    "\n",
    "        # Save predictions\n",
    "        predictions_df = pd.DataFrame({\n",
    "            'true_labels': all_true,\n",
    "            'predicted_scores': all_pred,\n",
    "            'predicted_binary': (all_pred > 0.5).astype(int)\n",
    "        })\n",
    "        predictions_df.to_csv('tuned_auc_predictions_10pct_l2.csv', index=False)\n",
    "\n",
    "        # Save training history\n",
    "        history_df = pd.DataFrame({\n",
    "            'epoch': range(len(train_losses)),\n",
    "            'train_loss': train_losses,\n",
    "            'val_f1': val_f1_scores[:len(train_losses)] + [None] * max(0, len(train_losses) - len(val_f1_scores)),\n",
    "            'val_auc': val_auc_scores[:len(train_losses)] + [None] * max(0, len(train_losses) - len(val_auc_scores))\n",
    "        })\n",
    "        history_df.to_csv('tuned_auc_training_history_10pct_l2.csv', index=False)\n",
    "\n",
    "        # Save model performance metrics\n",
    "        metrics_df = pd.DataFrame({\n",
    "            'metric': ['F1-Score', 'AUC-ROC', 'Average_Precision'],\n",
    "            'score': [f1, roc_auc, avg_precision]\n",
    "        })\n",
    "        metrics_df.to_csv('tuned_auc_metrics_10pct_l2.csv', index=False)\n",
    "\n",
    "        # Save ROC curve data\n",
    "        roc_df = pd.DataFrame({\n",
    "            'fpr': fpr,\n",
    "            'tpr': tpr\n",
    "        })\n",
    "        roc_df.to_csv('tuned_auc_roc_curve_10pct_l2.csv', index=False)\n",
    "\n",
    "        # Save precision-recall curve data\n",
    "        pr_df = pd.DataFrame({\n",
    "            'precision': precision[:-1],  # Remove last element which is undefined\n",
    "            'recall': recall[:-1]\n",
    "        })\n",
    "        pr_df.to_csv('tuned_auc_pr_curve_10pct_l2.csv', index=False)\n",
    "\n",
    "        # Save confusion matrix\n",
    "        cm_df = pd.DataFrame(cm,\n",
    "                           index=['Actual_Negative', 'Actual_Positive'],\n",
    "                           columns=['Predicted_Negative', 'Predicted_Positive'])\n",
    "        cm_df.to_csv('tuned_auc_confusion_matrix_10pct_l2.csv')\n",
    "\n",
    "        print(f\"\\nâœ… All data saved successfully:\")\n",
    "        print(f\"   - Predictions: tuned_auc_predictions_10pct_l2.csv\")\n",
    "        print(f\"   - Training history: tuned_auc_training_history_10pct_l2.csv\")\n",
    "        print(f\"   - Metrics: tuned_auc_metrics_10pct_l2.csv\")\n",
    "        print(f\"   - ROC curve: tuned_auc_roc_curve_10pct_l2.csv\")\n",
    "        print(f\"   - Precision-Recall curve: tuned_auc_pr_curve_10pct_l2.csv\")\n",
    "        print(f\"   - Confusion matrix: tuned_auc_confusion_matrix_10pct_l2.csv\")\n",
    "        print(f\"   - Comprehensive results: auc_tuned_comprehensive_results_10pct_l2.png\")\n",
    "        print(f\"   - Detailed analysis: auc_tuned_detailed_analysis_10pct_l2.png\")\n",
    "\n",
    "        if visualization_result is not None:\n",
    "            print(f\"   - Enhanced visualization: enhanced_auc_visualization_10pct_tuned_l2.png\")\n",
    "\n",
    "        print(f\"ðŸŽ‰ TUNED AUC-ENHANCED EXPERIMENT WITH 10% DATA AND L2 REGULARIZATION COMPLETED SUCCESSFULLY!\")\n",
    "\n",
    "        return {\n",
    "            'f1': f1,\n",
    "            'auc': roc_auc,\n",
    "            'avg_precision': avg_precision,\n",
    "            'train_losses': train_losses,\n",
    "            'val_f1_scores': val_f1_scores,\n",
    "            'val_auc_scores': val_auc_scores\n",
    "        }\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"\\nâŒ Error occurred: {e}\")\n",
    "        import traceback\n",
    "        traceback.print_exc()\n",
    "        return None\n",
    "\n",
    "    finally:\n",
    "        # Clean up GPU memory\n",
    "        torch.cuda.empty_cache()\n",
    "\n",
    "def main():\n",
    "    print(\"ðŸš€ TUNED AUC-ENHANCED 3-LAYER GraphSAGE Link Prediction - 10% Data (WITH L2 REGULARIZATION)\")\n",
    "    print(\"=\" * 70)\n",
    "\n",
    "    # Run tuned AUC-enhanced experiment for 10% data with L2 regularization\n",
    "    results = run_tuned_auc_experiment_10pct()\n",
    "\n",
    "    if results:\n",
    "        print(f\"\\nðŸŽ‰ FINAL TUNED AUC-ENHANCED RESULTS (10% Data WITH L2 REGULARIZATION):\")\n",
    "        print(f\"   F1-Score: {results['f1']:.4f}\")\n",
    "        print(f\"   AUC-ROC: {results['auc']:.4f}\")\n",
    "        print(f\"   Average Precision: {results['avg_precision']:.4f}\")\n",
    "\n",
    "        # Write comprehensive output file\n",
    "        with open('output_tuned_auc_10pct_l2.txt', 'w') as f:\n",
    "            f.write(\"TUNED AUC-ENHANCED 3-LAYER GraphSAGE - COMPREHENSIVE RESULTS (10% Data WITH L2 REGULARIZATION)\\n\")\n",
    "            f.write(\"=\" * 80 + \"\\n\\n\")\n",
    "\n",
    "            f.write(\"PERFORMANCE METRICS:\\n\")\n",
    "            f.write(\"-\" * 20 + \"\\n\")\n",
    "            f.write(f\"F1-Score: {results['f1']:.4f}\\n\")\n",
    "            f.write(f\"AUC-ROC: {results['auc']:.4f}\\n\")\n",
    "            f.write(f\"Average Precision: {results['avg_precision']:.4f}\\n\\n\")\n",
    "\n",
    "            f.write(\"COMPARISON WITH BASELINES:\\n\")\n",
    "            f.write(\"-\" * 25 + \"\\n\")\n",
    "            f.write(\"Model               | F1-Score | AUC-ROC  \\n\")\n",
    "            f.write(\"-\" * 25 + \"\\n\")\n",
    "            f.write(f\"Original GraphSAGE  | 0.517    | 0.785    \\n\")\n",
    "            f.write(f\"XGBoost            | 0.928    | 0.932    \\n\")\n",
    "            f.write(f\"5% Data GraphSAGE  | 0.860    | 0.723    \\n\")\n",
    "            f.write(f\"Prev 10% GraphSAGE | 0.757    | 0.695    \\n\")\n",
    "            f.write(f\"Tuned 10% Ours     | {results['f1']:.3f}    | {results['auc']:.3f}    \\n\\n\")\n",
    "\n",
    "            f.write(\"IMPROVEMENT ANALYSIS:\\n\")\n",
    "            f.write(\"-\" * 22 + \"\\n\")\n",
    "            improvement_f1 = ((results['f1'] - 0.517) / 0.517) * 100\n",
    "            improvement_auc = ((results['auc'] - 0.785) / 0.785) * 100\n",
    "            improvement_vs_5pct_f1 = ((results['f1'] - 0.860) / 0.860) * 100\n",
    "            improvement_vs_5pct_auc = ((results['auc'] - 0.723) / 0.723) * 100\n",
    "            improvement_vs_prev_10pct_f1 = ((results['f1'] - 0.757) / 0.757) * 100\n",
    "            improvement_vs_prev_10pct_auc = ((results['auc'] - 0.695) / 0.695) * 100\n",
    "\n",
    "            f.write(f\"F1-Score improvement vs Original: {improvement_f1:+.2f}%\\n\")\n",
    "            f.write(f\"AUC-ROC improvement vs Original: {improvement_auc:+.2f}%\\n\")\n",
    "            f.write(f\"F1-Score improvement vs 5% Data: {improvement_vs_5pct_f1:+.2f}%\\n\")\n",
    "            f.write(f\"AUC-ROC improvement vs 5% Data: {improvement_vs_5pct_auc:+.2f}%\\n\")\n",
    "            f.write(f\"F1-Score improvement vs Prev 10%: {improvement_vs_prev_10pct_f1:+.2f}%\\n\")\n",
    "            f.write(f\"AUC-ROC improvement vs Prev 10%: {improvement_vs_prev_10pct_auc:+.2f}%\\n\\n\")\n",
    "\n",
    "            f.write(\"EXPERIMENT DETAILS:\\n\")\n",
    "            f.write(\"-\" * 18 + \"\\n\")\n",
    "            f.write(f\"Training epochs: {len(results['train_losses'])}\\n\")\n",
    "            f.write(f\"Best validation AUC: {max(results['val_auc_scores']) if results['val_auc_scores'] else 'N/A':.4f}\\n\")\n",
    "            f.write(f\"Best validation F1: {max(results['val_f1_scores']) if results['val_f1_scores'] else 'N/A':.4f}\\n\")\n",
    "            f.write(f\"Model capacity: Increased (hidden=128, output=64)\\n\")\n",
    "            f.write(f\"Training regimen: Enhanced for 10% data with L2 regularization\\n\")\n",
    "            f.write(f\"L2 Lambda: 1e-5\\n\\n\")\n",
    "\n",
    "            f.write(\"OUTPUT FILES:\\n\")\n",
    "            f.write(\"-\" * 13 + \"\\n\")\n",
    "            f.write(\"âœ… tuned_auc_predictions_10pct_l2.csv - Model predictions\\n\")\n",
    "            f.write(\"âœ… tuned_auc_training_history_10pct_l2.csv - Training progress\\n\")\n",
    "            f.write(\"âœ… tuned_auc_metrics_10pct_l2.csv - Performance metrics\\n\")\n",
    "            f.write(\"âœ… tuned_auc_roc_curve_10pct_l2.csv - ROC curve data\\n\")\n",
    "            f.write(\"âœ… tuned_auc_pr_curve_10pct_l2.csv - Precision-Recall curve data\\n\")\n",
    "            f.write(\"âœ… tuned_auc_confusion_matrix_10pct_l2.csv - Confusion matrix\\n\")\n",
    "            f.write(\"âœ… auc_tuned_comprehensive_results_10pct_l2.png - Main results plot\\n\")\n",
    "            f.write(\"âœ… auc_tuned_detailed_analysis_10pct_l2.png - Detailed analysis\\n\")\n",
    "            f.write(\"âœ… enhanced_auc_visualization_10pct_tuned_l2.png - Enhanced graph visualization\\n\")\n",
    "            f.write(\"âœ… output_tuned_auc_10pct_l2.txt - This summary file\\n\")\n",
    "            f.write(\"âœ… best_auc_optimized_model_10pct_tuned_l2.pth - Saved model with L2 regularization\\n\")\n",
    "\n",
    "        print(f\"ðŸ“„ Comprehensive results written to: output_tuned_auc_10pct_l2.txt\")\n",
    "\n",
    "        # Final achievement summary\n",
    "        print(f\"\\nðŸ† KEY ACHIEVEMENTS WITH TUNED 10% DATA MODEL WITH L2 REGULARIZATION:\")\n",
    "        print(f\"   âœ“ F1-Score: {results['f1']:.4f} (Target: >0.86)\")\n",
    "        print(f\"   âœ“ AUC-ROC: {results['auc']:.4f} (Target: >0.80)\")\n",
    "        print(f\"   âœ“ L2 Regularization: Successfully implemented (lambda=1e-5)\")\n",
    "        print(f\"   âœ“ Improved model capacity for 10% data\")\n",
    "        print(f\"   âœ“ Enhanced training regimen with tuned parameters\")\n",
    "        print(f\"   âœ“ Fast feature computation maintained\")\n",
    "        print(f\"   âœ“ All outputs properly stored in files\")\n",
    "\n",
    "    else:\n",
    "        print(f\"\\nâŒ Experiment failed!\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
